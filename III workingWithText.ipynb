{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8919ad4-99b2-4cbb-b467-53da061ed8c5",
   "metadata": {},
   "source": [
    "Linear regression & nearest neighbor methods deal with the numerical values like prices, areas, number of oprtions.....\n",
    "\n",
    "How can we use textual information ....\n",
    "\n",
    "#### Natural Language Processing (NLP):\n",
    "\n",
    "Giving text a representation of the computers can work with is the foundation of NLP ...\n",
    "(NLP) comprises techniques that enable us to solve various tasks such as internet search, document categorization, and auto amtic question answering (Siri & Alexa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2213d2-49c3-4b43-bda0-27b01c67eebf",
   "metadata": {},
   "source": [
    "#### Bag-of_words matrix (BoW): \n",
    "\n",
    "is a fundamental concept in Natural Lannguage Processing(NLP) and text mining. It is a way of representing text data as numerical data, which can be used as input to machine Learning models.\n",
    "\n",
    "\n",
    "#### Key Concepts: \n",
    "\n",
    "1- Bag-of-Words Representations: \n",
    "   * in the bag of words model, text (such as sentence or document) is         represented as a collection (bag) of its words, disregarding grammer      and word order but keeping multiplicity. The model focuses on the         frequency of occurence of words within a document.\n",
    "     \n",
    "2- Vocabulary:\n",
    "   * The vocabulary is a set of all unique words found in the entire           corpus (The collection of documents or text data). Each unique word       in the vocabulary represents a column in the matrix.\n",
    "\n",
    "3- Document:\n",
    "   * A document is an individual piece of text (e.g sentence, paragraph,       or article) within the corpus. Each document corresponds in the row       matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a4e92-2a63-4aad-b688-a8b387fa52f7",
   "metadata": {},
   "source": [
    "#### How the Bag-of-words Matrix Constructed:\n",
    "\n",
    "1- Tokenization: \n",
    "   * Split each document into individual words(tpkens).\n",
    "\n",
    "2- Building the Vocabulary:\n",
    "   * Compile a list of all unique words from the entire corpus. This list      forms the columons of the matrix.\n",
    "\n",
    "3- Filling the matrix:\n",
    "   * For each document, count the occurences of each word from the             vocabulary.\n",
    "   * Each row from the matrix represents a document, and each column           represents a word from the vocabulary. The value in cell (i, j) of        the matrix presents the count of word 'j' in document 'i'.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3ae937-660b-473d-b1cc-a6f4ebe16e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# \"I Love Dogs\" ===> Document 1\n",
    "# \"Dogs Love Playing\" ===> Document 2 \n",
    "# STEP 1 Tokanization\n",
    "list1 = ['I', 'Love', 'Dogs']\n",
    "list2 = ['Dogs', 'Love', 'Playing']\n",
    "# STEP 2 Building the vocabulary\n",
    "vocab = ['I', 'Love', 'Dogs', 'Playing'] #الكلامات بدوت تكرار\n",
    "# STEP 3 Construct the Bag-of-Words matrix\n",
    "\n",
    "Bow_matrix = {\n",
    "    ' ': ['Doc1', 'Doc2'],\n",
    "    'I': [1, 0],\n",
    "    'Love': [1, 1],\n",
    "    'Dogs':[1,1],\n",
    "    'Playing':[0,1]\n",
    "}\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5901dece-45bc-409a-acc6-2ef075d971c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bow_matrix_ = pd.DataFrame(Bow_matrix, index =['',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c49c80-9d6b-48b3-b554-e5620359acf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>Love</th>\n",
       "      <th>Dogs</th>\n",
       "      <th>Playing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Doc1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Doc2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        I  Love  Dogs  Playing\n",
       "  Doc1  1     1     1        0\n",
       "  Doc2  0     1     1        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bow_matrix_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775096f6-b787-45c4-a0a8-a40af7c7219d",
   "metadata": {},
   "source": [
    "* In the matrix:\n",
    "  * Row 1 corresponds to \"I Love Dogs\" (document 1).\n",
    "  * Row 2 corresponds to \"Dogs Love Playing\" (documnet 2).\n",
    "  * Each cell contains the count of each word from the vocabulary in the      corresponding document.\n",
    "\n",
    "### Use Cases:\n",
    "  * Text Calssification: The Bag-of-Words matrix is often used as a           feature representation for text classification tasks, such as spam        detection, sentiment analysis, or topic categorization.\n",
    "  * Information Retrieval: It can be used in search engines to match          quires to documents based on word frequency.\n",
    "  * Clustering: The matrix can serve as input for clustering algorithms       to group similar documents togther.\n",
    "\n",
    "### Limitations:\n",
    "  * Lack of context: The Bag-of-Words model ignores word order and            context, which means it can't capture the meaning or semantics of the     text.\n",
    "  * High Dimenstionality: With a large vocabulary, the matrix can become      very large and sparse (mostely zeros) which can be computationally        very expensive to process.\n",
    "  * Handling Synonyms and Polysemy: The model treats different words with     similar meanings as completely distinct, and the same word in             different contexts might have different meanings.\n",
    "\n",
    "### Extensions:\n",
    "To address some of these limitations, other models and techniques are often used alongside or as alternatives to Bag-of-Words, such as:\n",
    "  *  _TF-IDF (Term Frequency-Inverse Document Frequency):_ A variant that     considers not just the frequency of words in document, but also           unique or common the word is across all documents.\n",
    "  *  _Word Embeddings:_ Represent words as dense vectors that capture         semantic meanings, allowing for more context-aware processing.\n",
    "  *  _N-grams:_ Consider not just individual words but also fixed             sequences of words (e.g, bigrams, trigrams) to capture more context.\n",
    "\n",
    "### Summary:\n",
    "The Bag-of-Words matrix is a simple and effective way to represent text data as numerical features, particularly useful for tasks where word frequency is important. Despite its limitations, it remains a founditional technique in text processing and is often the first step in more complex text analysis piplines.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fa4961-ecb1-4a87-a78b-4d0b11199a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_piggy1 = {\n",
    "    'this': [1,1,1,1,1],\n",
    "    'home': [0,1,0,0,1],\n",
    "    'way':  [0,0,0,0,1],\n",
    "    'to'  : [1,0,0,0,0],\n",
    "    'cried':[0,0,0,0,1],\n",
    "    'wee':  [0,0,0,0,3],\n",
    "    'went': [1,0,0,0,0],\n",
    "    'all':  [0,0,0,0,1],\n",
    "    'piggy':[1,1,1,1,1],\n",
    "    'stayed':[0,1,0,0,0],\n",
    "    'had':  [0,0,1,1,0],\n",
    "    'none': [0,0,0,1,0],\n",
    "    'beef': [0,0,1,0,0],\n",
    "    'and':  [0,0,0,0,1],\n",
    "    'the':  [0,0,0,0,1],\n",
    "    'roast':[0,0,1,0,0],\n",
    "    'market':[1,0,0,0,0],\n",
    "    'little':[1,1,1,1,1] \n",
    "}\n",
    "\n",
    "little_piggy2 = pd.DataFrame(little_piggy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0027d518-adc1-4b49-ac66-08d06580031c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>home</th>\n",
       "      <th>way</th>\n",
       "      <th>to</th>\n",
       "      <th>cried</th>\n",
       "      <th>wee</th>\n",
       "      <th>went</th>\n",
       "      <th>all</th>\n",
       "      <th>piggy</th>\n",
       "      <th>stayed</th>\n",
       "      <th>had</th>\n",
       "      <th>none</th>\n",
       "      <th>beef</th>\n",
       "      <th>and</th>\n",
       "      <th>the</th>\n",
       "      <th>roast</th>\n",
       "      <th>market</th>\n",
       "      <th>little</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  home  way  to  cried  wee  went  all  piggy  stayed  had  none  beef  \\\n",
       "0     1     0    0   1      0    0     1    0      1       0    0     0     0   \n",
       "1     1     1    0   0      0    0     0    0      1       1    0     0     0   \n",
       "2     1     0    0   0      0    0     0    0      1       0    1     0     1   \n",
       "3     1     0    0   0      0    0     0    0      1       0    1     1     0   \n",
       "4     1     1    1   0      1    3     0    1      1       0    0     0     0   \n",
       "\n",
       "   and  the  roast  market  little  \n",
       "0    0    0      0       1       1  \n",
       "1    0    0      0       0       1  \n",
       "2    0    0      1       0       1  \n",
       "3    0    0      0       0       1  \n",
       "4    1    1      0       0       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_piggy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e0c3f3-b34c-4d38-95a3-d12541c82dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5, 6, 5, 12], [5, 0, 5, 4, 9], [6, 5, 0, 3, 12], [5, 4, 3, 0, 11], [12, 9, 12, 11, 0]]\n"
     ]
    }
   ],
   "source": [
    "# this data here is the bag of words representation of This Little Piggy\n",
    "\n",
    "data = [\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
    "]\n",
    "\n",
    "def distance(row1, row2):\n",
    "    return sum(abs(a - b)for a,b in zip(row1, row2))\n",
    "    \n",
    "    # fix this function so that it returns \n",
    "    # the sum of differences between the occurrences\n",
    "    # of each word in row1 and row2.\n",
    "    # you can assume that row1 and row2 are lists with equal length, containing numeric values.\n",
    "    #return 0\n",
    "\n",
    "def all_pairs(data):\n",
    "    # this calls the distance function for all the two-row combinations in the data\n",
    "    # you do not need to change this\n",
    "    dist = [[distance(sent1, sent2) for sent1 in data] for sent2 in data]\n",
    "    print(dist)\n",
    "\n",
    "all_pairs(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9bf42e-f8cd-4a36-a06d-29521a18625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar lines are Line 3 and Line 4\n",
      "The minimum distance between them is: 3\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
    "]\n",
    "\n",
    "def distance(row1, row2):\n",
    "    return sum(abs(a - b) for a, b in zip(row1, row2))\n",
    "\n",
    "def find_most_similar_lines(data):\n",
    "    min_distance = float('inf')\n",
    "    most_similar_pair = None\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(i + 1, len(data)):\n",
    "            dist = distance(data[i], data[j])\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                most_similar_pair = (i, j)\n",
    "                \n",
    "    return most_similar_pair, min_distance\n",
    "\n",
    "most_similar_pair, min_distance = find_most_similar_lines(data)\n",
    "\n",
    "print(f\"The most similar lines are Line {most_similar_pair[0] + 1} and Line {most_similar_pair[1] + 1}\")\n",
    "print(f\"The minimum distance between them is: {min_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bf248-3a15-43fd-b758-608ba8aa74aa",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "* 'distance (row1, row2)': This function calculates the sum of absolute      difference between the elements of two lists (rows).\n",
    "* 'find_most_similar_lines(data)': This function compares each pair of       rows in 'data', calculates their distance using the 'distance'            function, and keeps track of the smallest distance found. The pair of     rows with the smallest distance found. The pair of rows with the          smallest distance are considered the most similar.\n",
    "* _Output:_ The indices of the most similar pair of lines and their          corresponding minimum distance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28900db-4a9e-4b80-b8be-41875cdab8fc",
   "metadata": {},
   "source": [
    "One problem with the Bag-of-Words approach is that we are basically placing equal importance on any word: ten occurences of the word \"a\" is just as important as ten occurences of the word \"magnificent\".  \n",
    "But if we were,for example, considering text sentiment (analysing how someone feels), \"a\" and \"magnificent\" are obviously not equally important.  \n",
    "One popular and effective solution that we can apply to improve on the simple bag-of-words representations is called by the acronym  \n",
    "\n",
    "\n",
    "### Term Frequency and Inversed Document Frequency(tf-idf).  \n",
    "This approach places more weight on occurences of infrequent words compared to the common words like \"a\", \"The\", \"is\" and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695836b-cb5e-4447-898e-c5334e6359a9",
   "metadata": {},
   "source": [
    "In its simplest form the algorithm is as follows:  \n",
    "1- $(tf)$ Calculate the frequency (the number of occurences divided by document     length) for each word in your collection of documents. This is the        \"term frequency\", or   $tf$. (Note: ignore punctiation and                capitalization when doing this).   \n",
    "\n",
    "$$ TF_{\\text(t,d)} = \\frac{Number of times term (t) appears in document(d)}{Total number of terms in document (d)} $$\n",
    "\n",
    "\n",
    "2- $(idf)$Calculate how many documents each word appears in, and divide this by     the total number of documents. This is the \"document frequency\", of       $df$. Since we wish to assign less wight to common words, we will use     the inverse of this, 1÷$df$ (مقلوب).    \n",
    "This measures how important a term is within the entire corpus.  \n",
    "The idea is to reduce the weight of terms that occur very frequently across all documents because they are less informative ...  \n",
    "* __IDF__ is calculated as:\n",
    "$$IDF_{\\text(t,D)} = log(\\frac{Total number of doucments (D)}{Number of documents containing term (t)})$$\n",
    "* Sometimes, 1 is added to the denominator to prevent division by zero\n",
    "  $$IDF_{\\text(t,D)} = log(\\frac{Total number of doucments (D)}{1 + Number of documents containing term (t)})$$  \n",
    "\n",
    "3- __tf-idf score__ There are different ways to combine these two numbers to assign           weights to each word. The most common is the product of the term          frequency and the algorithm of the inverse of the document frequency:  \n",
    "   $tf-idf = tf * log(1 ÷ df) $.  \n",
    "   \n",
    "   OR   \n",
    "   \n",
    "   $tf-idf = tf * (idf)$  \n",
    "a high TF-IDF score means the term is important to that particular document but not common across the entire corpus   \n",
    "\n",
    "\n",
    "This may sound complicated, but don't panic! It will make more sense with the example below.  \n",
    "Let's consider these three sentences:  \n",
    "1- \"He really, really loves coffee\"  \n",
    "2-\"My sister dislikes coffee\"  \n",
    "3- \"My sister loves tea\"   \n",
    "\n",
    "In the tf-idf terminology, each of the sentences is a document, and collectively they form out corpus.  \n",
    "To calculate it-idf representation for each document,   \n",
    "* we start by listing all the unique words we have in the corpus.     \n",
    "In this case they are: 'coffee', 'dislike', 'he', 'loves', 'my', 'really', 'sister', and 'tea'...  \n",
    "* Next we look at each document in turn, count the occurences of each word in it, and dividethe number by the length of the document. Remember: frequency is the rate of occurence, so the number of occurences divided by the total number of words in a document:  \n",
    "  Document 1: he $\\frac{1}{5}$, really $\\frac{2}{5}$, loves $\\frac{1}{5}$, coffee $\\frac{1}{5}$  \n",
    "  Document 2: my $\\frac{1}{4}$, sister $\\frac{1}{4}$, dislike $\\frac{1}{4}$, coffee $\\frac{1}{4}$  \n",
    "  documet 3: my $\\frac{1}{4}$, sister $\\frac{1}{4}$, loves $\\frac{1}{4}$, tea $\\frac{1}{4}$\n",
    "* Halfway there Next, let's calculate the document frequencies of each word is the number of documents that contain at least one occurence of the word:  \n",
    "  coffee $\\frac{2}{3}$, dislike $\\frac{1}{3}$, he $\\frac{1}{3}$, loves $\\frac{2}{3}$, my $\\frac{2}{3}$, really $\\frac{1}{3}$, sister $\\frac{2}{3}$, tea $\\frac{1}{3}$\n",
    "\n",
    "___Now all there is left is to calculate the __tf-idf__ by multiplying the term frequency with the logarithm of the inverse document frequency.___  \n",
    "\n",
    "take for example the word \"he\" in Document 1. The term frequency is $\\frac{1}{5}$ since the word appears once in a five words document, and the document frequency is $\\frac{1}{3}$ since the word appears in one out of three documents.  \n",
    "On the whole, the tf-idf value of \"he\" in Document 1is given by $\\frac{1}{5} * log(\\frac{3}{1})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8aa141-5813-4ae9-bc9f-0d9b91e13e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "1/5 * (math.log(3/1,10)) #0.09542425094393249 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fab82-daa2-4766-9c16-0a27399722d5",
   "metadata": {},
   "source": [
    "Let's use the Hupty Dumpty as your corpus ....  \n",
    "Humpty Dumpty sat on a wall.  \n",
    "Humpty Dumpty had a great fall.  \n",
    "All the king's horses and all the king's men.  \n",
    "Couldn't put Humpty togther again.  \n",
    "\n",
    "1- the term frequency (tf) for the word \"Humpty\" in line 1 is $\\frac{1}{6}$.  \n",
    "2- the term frequency (tf) for the word \"all\" in line 3 is $\\frac{2}{9}$.\n",
    "\n",
    "3- the documnet frequency (df) for the word \"Humpty\" in all the corpus is $\\frac{3}{4}$.    \n",
    "\n",
    "So, the tf-idf score for the word \"Humpty\" in line 4 of corpus is  \n",
    "tf-idf = $\\frac{1}{6}$ * $log(\\frac{3}{4}) ≈ 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c896b24-fa8f-4389-8cdb-e3d5848c240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.020823122768049988"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6 * (math.log(3/4, 10)) #-0.020823122768049988 ignore the positive mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d5fc4-ea3c-40fb-8a14-5eed31f01aea",
   "metadata": {},
   "source": [
    "After you have obtained a vector representation for your documents, you can utilize them in many different ways. Someclassical examples from text classification include detecting the sentiment(negative, positive) of a product reviews and the spamminess of an email messages (spam / not).  \n",
    "\n",
    "But what use is all of this? for any given sentence in any corpus you now have vector representation. Do you notice the similarity to how we represented cabins? For each cabin we had a vector whose components describe the sentence in some way. The point is we now classify text since it is represented by numbers instead of words.  \n",
    "\n",
    "Another way of obtaining vector representations of documents relies on word embeddings, which  are dense real number vector representation of words. We won't go into detail here on how these are produced here, but you can check (Word2vec).\n",
    "\n",
    "(word2vec) is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through word2vec have proven to be successful on a variety of downstream natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6393d7c-fcbb-4c47-9206-dd90baa19ed4",
   "metadata": {},
   "source": [
    "### Terminology:  \n",
    "1- Word vector representation: also known as word embedding, is a way of representing words in a continuous vector space, where each word is represented by a dense vector of real numbers.  \n",
    "This method captures the semintic meaning of words based on their context on corpus, meaning that words that have the similar meaning tend to have similar vector representations.  \n",
    "\n",
    "2- Continuous vector space: refers to a mathimatical space where vectors, which represent entities like words in Natural Language Processing(NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bbeb3a-4240-480e-915e-6c1575e5bb2c",
   "metadata": {},
   "source": [
    "## Case:\n",
    "### Somebuddy - support for victims of online haressment\n",
    "\n",
    "in order to serve people quickly and scalably, the system created by (SomeBuddy) and (Reaktor) is based on (Natural Language Processing) to process people's answers to an online questionnare. In a vast majority of cases, the system can tell if a crime has been commited or not and pre-generate information that can be further elaborated on by by SomeBuddy's legal and psychological team.  \n",
    "\n",
    "In this way, machine learningis used to support the work of knowlegable and empathetic people at SomeBuddy, who inturn provide support to vicitms of online harassment.  \n",
    "__AI method used__  \n",
    "• Natural Language Processin (NLP).  \n",
    "• Word embeddings.  \n",
    "• Neural Network for classification.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae80e66-4184-48d4-8003-57f44cf10850",
   "metadata": {},
   "source": [
    "$$Great Good Bye $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330093fc-4932-4070-b5be-cacad7a2abcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
